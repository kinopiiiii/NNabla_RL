{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Use Cuda</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cart Pole Environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Epsilon greedy exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa6b558cc50>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deep Q Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Computing Temporal Difference Loss</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/near/RL-a/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0, device='cuda:0') (<class 'torch.Tensor'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-cac98d1999f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-a/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL-a/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%r (%s) invalid\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0, device='cuda:0') (<class 'torch.Tensor'>) invalid"
     ]
    }
   ],
   "source": [
    "num_frames = 10000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><hr></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atari Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), volatile=True)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(1000000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 1400000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(batch_size)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-a",
   "language": "python",
   "name": "rl-a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
